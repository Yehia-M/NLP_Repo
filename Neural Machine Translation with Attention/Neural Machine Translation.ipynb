{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "English-to-German neural machine translation (NMT) model using Long Short-Term Memory (LSTM) networks with attention.  Machine translation is an important task in natural language processing and could be useful not only for translating one language to another but also for word sense disambiguation (e.g. determining whether the word \"bank\" refers to the financial bank, or the land alongside a river). Implementing this using just a Recurrent Neural Network (RNN) with LSTMs can work for short to medium length sentences but can result in vanishing gradients for very long sequences. To solve this, an attention mechanism is added to allow the decoder to access all relevant parts of the input sentence regardless of its length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:tokens_length=568 inputs_length=512 targets_length=114 noise_density=0.15 mean_noise_span_length=3.0 \n",
      "trax                     1.3.4\n",
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from termcolor import colored\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import trax\n",
    "from trax import layers as tl\n",
    "from trax.fastmath import numpy as fastnp\n",
    "from trax.supervised import training\n",
    "\n",
    "!pip list | grep trax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will import the dataset we will use to train the model. To meet the storage constraints in this lab environment, we will just use a small dataset from [Opus](http://opus.nlpl.eu/), a growing collection of translated texts from the web. Particularly, we will get an English to German translation subset specified as `opus/medical` which has medical related texts. If storage is not an issue, you can opt to get a larger corpus such as the English to German translation dataset from [ParaCrawl](https://paracrawl.eu/), a large multi-lingual translation dataset created by the European Union. Both of these datasets are available via [Tensorflow Datasets (TFDS)](https://www.tensorflow.org/datasets)\n",
    "and you can browse through the other available datasets [here](https://www.tensorflow.org/datasets/catalog/overview).The result is a python generator function yielding tuples. Use the `keys` argument to select what appears at which position in the tuple. For example, `keys=('en', 'de')` below will return pairs as (English sentence, German sentence).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get generator function for the training set\n",
    "# This will download the train dataset if no data_dir is specified.\n",
    "train_stream_fn = trax.data.TFDS('opus/medical',\n",
    "                                 data_dir='./data/',\n",
    "                                 keys=('en', 'de'),\n",
    "                                 eval_holdout_size=0.01, # 1% for eval\n",
    "                                 train=True)\n",
    "\n",
    "# Get generator function for the eval set\n",
    "eval_stream_fn = trax.data.TFDS('opus/medical',\n",
    "                                data_dir='./data/',\n",
    "                                keys=('en', 'de'),\n",
    "                                eval_holdout_size=0.01, # 1% for eval\n",
    "                                train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtrain data (en, de) tuple:\u001b[0m (b'4.\\n', b'4.\\n')\n",
      "\n",
      "\u001b[31meval data (en, de) tuple:\u001b[0m (b'Lutropin alfa Subcutaneous use.\\n', b'Pulver zur Injektion Lutropin alfa Subkutane Anwendung\\n')\n"
     ]
    }
   ],
   "source": [
    "train_stream = train_stream_fn()\n",
    "print(colored('train data (en, de) tuple:', 'red'), next(train_stream))\n",
    "print()\n",
    "\n",
    "eval_stream = eval_stream_fn()\n",
    "print(colored('eval data (en, de) tuple:', 'red'), next(eval_stream))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1.2\"></a>\n",
    "## Tokenization and Formatting\n",
    "\n",
    "Now that we have imported our corpus, we will be preprocessing the sentences into a format that our model can accept. This will be composed of several steps:\n",
    "\n",
    "**Tokenizing the sentences using subword representations:** We want to represent each sentence as an array of integers instead of strings. For our application, we will use *subword* representations to tokenize our sentences. This is a common technique to avoid out-of-vocabulary words by allowing parts of words to be represented separately. For example, instead of having separate entries in your vocabulary for --\"fear\", \"fearless\", \"fearsome\", \"some\", and \"less\"--, you can simply store --\"fear\", \"some\", and \"less\"-- then allow your tokenizer to combine these subwords when needed. This allows it to be more flexible so you won't have to save uncommon words explicitly in your vocabulary (e.g. *stylebender*, *nonce*, etc). Tokenizing is done with the `trax.data.Tokenize()` command and we have provided you the combined subword vocabulary for English and German (i.e. `ende_32k.subword`) saved in the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables that state the filename and directory of the vocabulary file\n",
    "VOCAB_FILE = 'ende_32k.subword'\n",
    "VOCAB_DIR = 'data/'\n",
    "\n",
    "# Tokenize the dataset.\n",
    "tokenized_train_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(train_stream)\n",
    "tokenized_eval_stream = trax.data.Tokenize(vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)(eval_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append EOS at the end of each sentence.\n",
    "EOS = 1\n",
    "\n",
    "# generator helper function to append EOS to each sentence\n",
    "def append_eos(stream):\n",
    "    for (inputs, targets) in stream:\n",
    "        inputs_with_eos = list(inputs) + [EOS]\n",
    "        targets_with_eos = list(targets) + [EOS]\n",
    "        yield np.array(inputs_with_eos), np.array(targets_with_eos)\n",
    "\n",
    "# append EOS to the train data\n",
    "tokenized_train_stream = append_eos(tokenized_train_stream)\n",
    "\n",
    "# append EOS to the eval data\n",
    "tokenized_eval_stream = append_eos(tokenized_eval_stream)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter long sentences:** We will place a limit on the number of tokens per sentence to ensure we won't run out of memory. This is done with the `trax.data.FilterByLength()` method and you can see its syntax below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSingle tokenized example input:\u001b[0m [10652 30650  4729   992     1]\n",
      "\u001b[31mSingle tokenized example target:\u001b[0m [14292 30650  4729   992     1]\n"
     ]
    }
   ],
   "source": [
    "# Filter too long sentences to not run out of memory.\n",
    "# length_keys=[0, 1] means we filter both English and German sentences, so\n",
    "# both much be not longer that 256 tokens for training / 512 for eval.\n",
    "filtered_train_stream = trax.data.FilterByLength(\n",
    "    max_length=256, length_keys=[0, 1])(tokenized_train_stream)\n",
    "filtered_eval_stream = trax.data.FilterByLength(\n",
    "    max_length=512, length_keys=[0, 1])(tokenized_eval_stream)\n",
    "\n",
    "# print a sample input-target pair of tokenized sentences\n",
    "train_input, train_target = next(filtered_train_stream)\n",
    "print(colored(f'Single tokenized example input:', 'red' ), train_input)\n",
    "print(colored(f'Single tokenized example target:', 'red'), train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(input_str, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"Encodes a string to an array of integers\n",
    "\n",
    "    Args:\n",
    "        input_str (str): human-readable string to encode\n",
    "        vocab_file (str): filename of the vocabulary text file\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "  \n",
    "    Returns:\n",
    "        numpy.ndarray: tokenized version of the input string\n",
    "    \"\"\"\n",
    "    \n",
    "    # Set the encoding of the \"end of sentence\" as 1\n",
    "    EOS = 1\n",
    "    \n",
    "    inputs =  next(trax.data.tokenize(iter([input_str]),\n",
    "                                      vocab_file=vocab_file, vocab_dir=vocab_dir))\n",
    "    \n",
    "    # Mark the end of the sentence with EOS\n",
    "    inputs = list(inputs) + [EOS]\n",
    "    \n",
    "    # Adding the batch dimension to the front of the shape\n",
    "    batch_inputs = np.reshape(np.array(inputs), [1, -1])\n",
    "    \n",
    "    return batch_inputs\n",
    "\n",
    "\n",
    "def detokenize(integers, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"Decodes an array of integers to a human readable string\n",
    "\n",
    "    Args:\n",
    "        integers (numpy.ndarray): array of integers to decode\n",
    "        vocab_file (str): filename of the vocabulary text file\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "  \n",
    "    Returns:\n",
    "        str: the decoded sentence.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Remove the dimensions of size 1\n",
    "    integers = list(np.squeeze(integers))\n",
    "    \n",
    "    # Set the encoding of the \"end of sentence\" as 1\n",
    "    EOS = 1\n",
    "    \n",
    "    # Remove the EOS to decode only the original tokens\n",
    "    if EOS in integers:\n",
    "        integers = integers[:integers.index(EOS)] \n",
    "    \n",
    "    return trax.data.detokenize(integers, vocab_file=vocab_file, vocab_dir=vocab_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSingle detokenized example input:\u001b[0m Finland\n",
      "\n",
      "\u001b[31mSingle detokenized example target:\u001b[0m Finnland\n",
      "\n",
      "\n",
      "\u001b[32mtokenize('hello'): \u001b[0m [[17332   140    14    46     1]]\n",
      "\u001b[32mdetokenize([17332, 140, 1]): \u001b[0m hello\n"
     ]
    }
   ],
   "source": [
    "# As declared earlier:\n",
    "# VOCAB_FILE = 'ende_32k.subword'\n",
    "# VOCAB_DIR = 'data/'\n",
    "\n",
    "# Detokenize an input-target pair of tokenized sentences\n",
    "print(colored(f'Single detokenized example input:', 'red'), detokenize(train_input, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f'Single detokenized example target:', 'red'), detokenize(train_target, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print()\n",
    "\n",
    "# Tokenize and detokenize a word that is not explicitly saved in the vocabulary file.\n",
    "# See how it combines the subwords -- 'hell' and 'o'-- to form the word 'hello'.\n",
    "print(colored(f\"tokenize('hello'): \", 'green'), tokenize('hello s I', vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))\n",
    "print(colored(f\"detokenize([17332, 140, 1]): \", 'green'), detokenize([17332, 140, 1], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"1.4\"></a>\n",
    "## Bucketing\n",
    "\n",
    "Bucketing the tokenized sentences is an important technique used to speed up training in NLP.\n",
    "Our inputs have variable lengths and you want to make these the same when batching groups of sentences together. One way to do that is to pad each sentence to the length of the longest sentence in the dataset. This might lead to some wasted computation though. For example, if there are multiple short sentences with just two tokens, do we want to pad these when the longest sentence is composed of a 100 tokens? Instead of padding with 0s to the maximum length of a sentence each time, we can group our tokenized sentences by length and bucket.\n",
    "\n",
    "We batch the sentences with similar length together and only add minimal padding to make them have equal length (usually up to the nearest power of two). This allows to waste less computation when processing padded sequences.\n",
    "In Trax, it is implemented in the [bucket_by_length](https://github.com/google/trax/blob/5fb8aa8c5cb86dabb2338938c745996d5d87d996/trax/supervised/inputs.py#L378) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bucketing to create streams of batches.\n",
    "\n",
    "# Buckets are defined in terms of boundaries and batch sizes.\n",
    "# Batch_sizes[i] determines the batch size for items with length < boundaries[i]\n",
    "# So below, we'll take a batch of 256 sentences of length < 8, 128 if length is\n",
    "# between 8 and 16, and so on -- and only 2 if length is over 512.\n",
    "boundaries =  [8,   16,  32, 64, 128, 256, 512]\n",
    "batch_sizes = [256, 128, 64, 32, 16,    8,   4,  2]\n",
    "\n",
    "# Create the generators.\n",
    "train_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes,\n",
    "    length_keys=[0, 1]  # As before: count inputs and targets to length.\n",
    ")(filtered_train_stream)\n",
    "\n",
    "eval_batch_stream = trax.data.BucketByLength(\n",
    "    boundaries, batch_sizes,\n",
    "    length_keys=[0, 1]  # As before: count inputs and targets to length.\n",
    ")(filtered_eval_stream)\n",
    "\n",
    "# Add masking for the padding (0s).\n",
    "train_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(train_batch_stream)\n",
    "eval_batch_stream = trax.data.AddLossWeights(id_to_mask=0)(eval_batch_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch data type:  <class 'numpy.ndarray'>\n",
      "target_batch data type:  <class 'numpy.ndarray'>\n",
      "input_batch shape:  (32, 64)\n",
      "target_batch shape:  (32, 64)\n"
     ]
    }
   ],
   "source": [
    "input_batch, target_batch, mask_batch = next(train_batch_stream)\n",
    "\n",
    "# let's see the data type of a batch\n",
    "print(\"input_batch data type: \", type(input_batch))\n",
    "print(\"target_batch data type: \", type(target_batch))\n",
    "\n",
    "# let's see the shape of this particular batch (batch length, sentence length)\n",
    "print(\"input_batch shape: \", input_batch.shape)\n",
    "print(\"target_batch shape: \", target_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_batch:  [[   71     4  3678 ...     0     0     0]\n",
      " [  176 11086     7 ...     0     0     0]\n",
      " [   29   197 24445 ...     0     0     0]\n",
      " ...\n",
      " [ 7478 11133    14 ...     0     0     0]\n",
      " [32523  2257  2646 ...     0     0     0]\n",
      " [  308 14471  1581 ...     0     0     0]]\n",
      "target_batch:  [[  752 22482 13831 ...     0     0     0]\n",
      " [  478  4470 12279 ...     0     0     0]\n",
      " [   57   131 20052 ...     0     0     0]\n",
      " ...\n",
      " [ 5122    39 30013 ...     1     0     0]\n",
      " [   57  2646 13309 ...     0     0     0]\n",
      " [ 1316 14471  1581 ...     0     0     0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"input_batch: \", input_batch)\n",
    "print(\"target_batch: \", target_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `input_batch` and `target_batch` are Numpy arrays consisting of tokenized English sentences and German sentences respectively. These tokens will later be used to produce embedding vectors for each word in the sentence (so the embedding for a sentence will be a matrix). The number of sentences in each batch is usually a power of 2 for optimal computer memory usage. \n",
    "\n",
    "We can now visually inspect some of the data. You can run the cell below several times to shuffle through the sentences. Just to note, while this is a standard data set that is used widely, it does have some known wrong translations. With that, let's pick a random sentence and print its tokenized representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mTHIS IS THE ENGLISH SENTENCE: \n",
      "\u001b[0m The adjusted mean difference was -4.3 points (CI 95% -6.4; -2.1 points, p-value < 0.0001).\n",
      " \n",
      "\n",
      "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \n",
      " \u001b[0m [   29  9701  1516  2640    53  1581   219     3   199  1164    50  7082\n",
      "     5  4207 11767    15   330     3   219  7108    15   150     3   135\n",
      "  1164     2   719    15   980   909 33287   913   266     3  8074  3912\n",
      " 33022 30650  4729   992     1     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n",
      "\u001b[31mTHIS IS THE GERMAN TRANSLATION: \n",
      "\u001b[0m Die angepasste mittlere Differenz betrug -4,3 Punkte (95 %-Konfidenzintervall: -6,4 bis -2,1 Punkte, p-Wert < 0,0001).\n",
      " \n",
      "\n",
      "\u001b[31mTHIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \n",
      "\u001b[0m [   57 30482  8385   191 14998     5 12919 20657  1581   219   227   199\n",
      "  2927    50  4207 11770    15 11580  7770 13427  9436 19070     5  2801\n",
      "    15   330   227   219   248  1581   150   227   135  2927     2   719\n",
      "    15  1619   909 33287   913   266   227  8074  3912 33022 30650  4729\n",
      "   992     1     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# pick a random index less than the batch size.\n",
    "index = random.randrange(len(input_batch))\n",
    "\n",
    "# use the index to grab an entry from the input and target batch\n",
    "print(colored('THIS IS THE ENGLISH SENTENCE: \\n', 'red'), detokenize(input_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE ENGLISH SENTENCE: \\n ', 'red'), input_batch[index], '\\n')\n",
    "print(colored('THIS IS THE GERMAN TRANSLATION: \\n', 'red'), detokenize(target_batch[index], vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR), '\\n')\n",
    "print(colored('THIS IS THE TOKENIZED VERSION OF THE GERMAN TRANSLATION: \\n', 'red'), target_batch[index], '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2\"></a>\n",
    "# Neural Machine Translation with Attention\n",
    "\n",
    "Now that we have the data generators and have handled the preprocessing, it is time to build the model from scratch with attention.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.1\"></a>\n",
    "## Attention Overview\n",
    "\n",
    "The model we will be building uses an encoder-decoder architecture. This Recurrent Neural Network (RNN) will take in a tokenized version of a sentence in its encoder, then passes it on to the decoder for translation. As mentioned in the lectures, just using a a regular sequence-to-sequence model with LSTMs will work effectively for short to medium sentences but will start to degrade for longer ones. You can picture it like the figure below where all of the context of the input sentence is compressed into one vector that is passed into the decoder block. You can see how this will be an issue for very long sentences (e.g. 100 tokens or more) because the context of the first parts of the input will have very little effect on the final vector passed to the decoder.\n",
    "\n",
    "<img src='plain_rnn.png'>\n",
    "\n",
    "Adding an attention layer to this model avoids this problem by giving the decoder access to all parts of the input sentence. To illustrate, let's just use a 4-word input sentence as shown below. Remember that a hidden state is produced at each timestep of the encoder (represented by the orange rectangles). These are all passed to the attention layer and each are given a score given the current activation (i.e. hidden state) of the decoder. For instance, let's consider the figure below where the first prediction \"Wie\" is already made. To produce the next prediction, the attention layer will first receive all the encoder hidden states (i.e. orange rectangles) as well as the decoder hidden state when producing the word \"Wie\" (i.e. first green rectangle). Given these information, it will score each of the encoder hidden states to know which one the decoder should focus on to produce the next word. The result of the model training might have learned that it should align to the second encoder hidden state and subsequently assigns a high probability to the word \"geht\". If we are using greedy decoding, we will output the said word as the next symbol, then restart the process to produce the next word until we reach an end-of-sentence prediction.\n",
    "\n",
    "<img src='attention_overview.png'>\n",
    "\n",
    "\n",
    "There are different ways to implement attention and the one we'll use for this assignment is the Scaled Dot Product Attention which has the form:\n",
    "\n",
    "$$Attention(Q, K, V) = softmax(\\frac{QK^T}{\\sqrt{d_k}})V$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.2\"></a>\n",
    "## Helper functions\n",
    "\n",
    "We will first implement a few functions that we will use later on. These will be for the input encoder, pre-attention decoder, and preparation of the queries, keys, values, and mask.\n",
    "\n",
    "### 1 Input encoder\n",
    "\n",
    "The input encoder runs on the input tokens, creates its embeddings, and feeds it to an LSTM network. This outputs the activations that will be the keys and values for attention. \n",
    "   \n",
    "<img src = \"input_encoder.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_encoder_fn(input_vocab_size, d_model, n_encoder_layers):\n",
    "    \"\"\" Input encoder runs on the input sentence and creates\n",
    "    activations that will be the keys and values for attention.\n",
    "    \n",
    "    Args:\n",
    "        input_vocab_size: int: vocab size of the input\n",
    "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
    "        n_encoder_layers: int: number of LSTM layers in the encoder\n",
    "    Returns:\n",
    "        tl.Serial: The input encoder\n",
    "    \"\"\"\n",
    "    \n",
    "    # create a serial network\n",
    "    input_encoder = tl.Serial( \n",
    "        tl.Embedding(input_vocab_size,d_model),\n",
    "        [tl.LSTM(d_model) for _ in range(n_encoder_layers)]\n",
    "    )\n",
    "\n",
    "    return input_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# BEGIN UNIT TEST\n",
    "import w1_unittest\n",
    "\n",
    "w1_unittest.test_input_encoder_fn(input_encoder_fn)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Pre-attention decoder\n",
    "\n",
    "The pre-attention decoder runs on the targets and creates activations that are used as queries in attention.\n",
    "\n",
    "<img src = \"pre_attention_decoder.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_attention_decoder_fn(mode, target_vocab_size, d_model):\n",
    "    \"\"\" Pre-attention decoder runs on the targets and creates\n",
    "    activations that are used as queries in attention.\n",
    "    \n",
    "    Args:\n",
    "        mode: str: 'train' or 'eval'\n",
    "        target_vocab_size: int: vocab size of the target\n",
    "        d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
    "    Returns:\n",
    "        tl.Serial: The pre-attention decoder\n",
    "    \"\"\"\n",
    "    \n",
    "    # create a serial network\n",
    "    pre_attention_decoder = tl.Serial(\n",
    "        \n",
    "        # shift right to insert start-of-sentence token\n",
    "        tl.ShiftRight(mode = mode),\n",
    "        tl.Embedding(target_vocab_size,d_model),\n",
    "        tl.LSTM(d_model)\n",
    "    )\n",
    "    \n",
    "    return pre_attention_decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# BEGIN UNIT TEST\n",
    "\n",
    "w1_unittest.test_pre_attention_decoder_fn(pre_attention_decoder_fn)\n",
    "\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Preparing the attention input\n",
    "\n",
    "This function will prepare the inputs to the attention layer. We want to take in the encoder and pre-attention decoder activations and assign it to the queries, keys, and values. In addition, another output here will be the mask to distinguish real tokens from padding tokens. This mask will be used internally by Trax when computing the softmax so padding tokens will not have an effect on the computated probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_attention_input(encoder_activations, decoder_activations, inputs):\n",
    "    \"\"\"Prepare queries, keys, values and mask for attention.\n",
    "    \n",
    "    Args:\n",
    "        encoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the input encoder\n",
    "        decoder_activations fastnp.array(batch_size, padded_input_length, d_model): output from the pre-attention decoder\n",
    "        inputs fastnp.array(batch_size, padded_input_length): padded input tokens\n",
    "    \n",
    "    Returns:\n",
    "        queries, keys, values and mask for attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    # set the keys and values to the encoder activations\n",
    "    keys = encoder_activations\n",
    "    values = encoder_activations\n",
    "\n",
    "    # set the queries to the decoder activations\n",
    "    queries = decoder_activations\n",
    "    \n",
    "    # generate the mask to distinguish real tokens from padding\n",
    "    mask = fastnp.array(inputs != 0)\n",
    "    \n",
    "    # add axes to the mask for attention heads and decoder length.\n",
    "    mask = fastnp.reshape(mask, (mask.shape[0], 1, 1, mask.shape[1]))\n",
    "    \n",
    "    # broadcast so mask shape is [batch size, attention heads, decoder-len, encoder-len].\n",
    "    mask = mask + fastnp.zeros((1, 1, decoder_activations.shape[1], 1))\n",
    "    \n",
    "    return queries, keys, values, mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_prepare_attention_input(prepare_attention_input)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"2.3\"></a>\n",
    "## Implementation Overview\n",
    "\n",
    "We are now ready to implement our sequence-to-sequence model with attention. This will be a Serial network and is illustrated in the diagram below. It shows the layers you'll be using in Trax\n",
    "\n",
    "<img src = \"NMTModel.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NMTAttn(input_vocab_size=33300,\n",
    "            target_vocab_size=33300,\n",
    "            d_model=1024,\n",
    "            n_encoder_layers=2,\n",
    "            n_decoder_layers=2,\n",
    "            n_attention_heads=4,\n",
    "            attention_dropout=0.0,\n",
    "            mode='train'):\n",
    "    \"\"\"Returns an LSTM sequence-to-sequence model with attention.\n",
    "\n",
    "    The input to the model is a pair (input tokens, target tokens), e.g.,\n",
    "    an English sentence (tokenized) and its translation into German (tokenized).\n",
    "\n",
    "    Args:\n",
    "    input_vocab_size: int: vocab size of the input\n",
    "    target_vocab_size: int: vocab size of the target\n",
    "    d_model: int:  depth of embedding (n_units in the LSTM cell)\n",
    "    n_encoder_layers: int: number of LSTM layers in the encoder\n",
    "    n_decoder_layers: int: number of LSTM layers in the decoder after attention\n",
    "    n_attention_heads: int: number of attention heads\n",
    "    attention_dropout: float, dropout for the attention layer\n",
    "    mode: str: 'train', 'eval' or 'predict', predict mode is for fast inference\n",
    "\n",
    "    Returns:\n",
    "    A LSTM sequence-to-sequence model with attention.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 0: call the helper function to create layers for the input encoder\n",
    "    input_encoder = input_encoder_fn(input_vocab_size, d_model, n_encoder_layers)\n",
    "\n",
    "    # Step 0: call the helper function to create layers for the pre-attention decoder\n",
    "    pre_attention_decoder = pre_attention_decoder_fn(mode, target_vocab_size, d_model)\n",
    "\n",
    "    # Step 1: create a serial network\n",
    "    model = tl.Serial( \n",
    "        \n",
    "      # Step 2: copy input tokens and target tokens as they will be needed later.\n",
    "      tl.Select([0,1,0,1]),\n",
    "        \n",
    "      # Step 3: run input encoder on the input and pre-attention decoder the target.\n",
    "      tl.Parallel(input_encoder, pre_attention_decoder),\n",
    "        \n",
    "      # Step 4: prepare queries, keys, values and mask for attention.\n",
    "      tl.Fn('PrepareAttentionInput', prepare_attention_input, n_out=4),\n",
    "        \n",
    "      # Step 5: run the AttentionQKV layer\n",
    "      # nest it inside a Residual layer to add to the pre-attention decoder activations\n",
    "      tl.Residual(tl.AttentionQKV(d_model, n_heads=n_attention_heads, dropout=attention_dropout, mode=mode)),\n",
    "      \n",
    "      # Step 6: drop attention mask\n",
    "      tl.Select([0,2]),\n",
    "        \n",
    "      # Step 7: run the rest of the RNN decoder\n",
    "      [tl.LSTM(d_model) for _ in range(n_decoder_layers)],\n",
    "        \n",
    "      # Step 8: prepare output by making it the right size\n",
    "      tl.Dense(target_vocab_size),\n",
    "        \n",
    "      # Step 9: Log-softmax for output\n",
    "      tl.LogSoftmax()\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_NMTAttn(NMTAttn)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serial_in2_out2[\n",
      "  Select[0,1,0,1]_in2_out4\n",
      "  Parallel_in2_out2[\n",
      "    Serial[\n",
      "      Embedding_33300_1024\n",
      "      LSTM_1024\n",
      "      LSTM_1024\n",
      "    ]\n",
      "    Serial[\n",
      "      ShiftRight(1)\n",
      "      Embedding_33300_1024\n",
      "      LSTM_1024\n",
      "    ]\n",
      "  ]\n",
      "  PrepareAttentionInput_in3_out4\n",
      "  Serial_in4_out2[\n",
      "    Branch_in4_out3[\n",
      "      None\n",
      "      Serial_in4_out2[\n",
      "        Parallel_in3_out3[\n",
      "          Dense_1024\n",
      "          Dense_1024\n",
      "          Dense_1024\n",
      "        ]\n",
      "        PureAttention_in4_out2\n",
      "        Dense_1024\n",
      "      ]\n",
      "    ]\n",
      "    Add_in2\n",
      "  ]\n",
      "  Select[0,2]_in3_out2\n",
      "  LSTM_1024\n",
      "  LSTM_1024\n",
      "  Dense_33300\n",
      "  LogSoftmax\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# print your model\n",
    "model = NMTAttn()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "```\n",
    "Serial_in2_out2[\n",
    "  Select[0,1,0,1]_in2_out4\n",
    "  Parallel_in2_out2[\n",
    "    Serial[\n",
    "      Embedding_33300_1024\n",
    "      LSTM_1024\n",
    "      LSTM_1024\n",
    "    ]\n",
    "    Serial[\n",
    "      ShiftRight(1)\n",
    "      Embedding_33300_1024\n",
    "      LSTM_1024\n",
    "    ]\n",
    "  ]\n",
    "  PrepareAttentionInput_in3_out4\n",
    "  Serial_in4_out2[\n",
    "    Branch_in4_out3[\n",
    "      None\n",
    "      Serial_in4_out2[\n",
    "        Parallel_in3_out3[\n",
    "          Dense_1024\n",
    "          Dense_1024\n",
    "          Dense_1024\n",
    "        ]\n",
    "        PureAttention_in4_out2\n",
    "        Dense_1024\n",
    "      ]\n",
    "    ]\n",
    "    Add_in2\n",
    "  ]\n",
    "  Select[0,2]_in3_out2\n",
    "  LSTM_1024\n",
    "  LSTM_1024\n",
    "  Dense_33300\n",
    "  LogSoftmax\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.1\"></a>\n",
    "## TrainTask\n",
    "\n",
    "The [TrainTask](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.TrainTask) class allows us to define the labeled data to use for training and the feedback mechanisms to compute the loss and update the weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task = training.TrainTask(\n",
    "    \n",
    "    # use the train batch stream as labeled data\n",
    "    labeled_data= train_batch_stream,\n",
    "    \n",
    "    loss_layer=tl.CrossEntropyLoss(),\n",
    "    optimizer= trax.optimizers.Adam(0.1),\n",
    "    lr_schedule= trax.lr.warmup_and_rsqrt_decay(1000,0.01),\n",
    "    n_steps_per_checkpoint= 10,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_train_task(train_task)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.2\"></a>\n",
    "## EvalTask\n",
    "\n",
    "The [EvalTask](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.EvalTask) on the other hand allows us to see how the model is doing while training. For our application, we want it to report the cross entropy loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_task = training.EvalTask(\n",
    "    labeled_data=eval_batch_stream,\n",
    "    metrics=[tl.CrossEntropyLoss(), tl.Accuracy()],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"3.3\"></a>\n",
    "## Loop\n",
    "\n",
    "The [Loop](https://trax-ml.readthedocs.io/en/latest/trax.supervised.html#trax.supervised.training.Loop) class defines the model we will train as well as the train and eval tasks to execute. Its `run()` method allows us to execute the training for a specified number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# define the output directory\n",
    "output_dir = 'output_dir/'\n",
    "\n",
    "# remove old model if it exists. restarts training.\n",
    "!rm -f ~/output_dir/model.pkl.gz  \n",
    "\n",
    "# define the training loop\n",
    "training_loop = training.Loop(NMTAttn(mode='train'),\n",
    "                              train_task,\n",
    "                              eval_tasks=[eval_task],\n",
    "                              output_dir=output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step      1: Ran 1 train steps in 139.22 secs\n",
      "Step      1: train CrossEntropyLoss |  10.40735626\n",
      "Step      1: eval  CrossEntropyLoss |  10.39813709\n",
      "Step      1: eval          Accuracy |  0.00000000\n",
      "\n",
      "Step     10: Ran 9 train steps in 465.83 secs\n",
      "Step     10: train CrossEntropyLoss |  10.25226688\n",
      "Step     10: eval  CrossEntropyLoss |  9.96502113\n",
      "Step     10: eval          Accuracy |  0.03333334\n"
     ]
    }
   ],
   "source": [
    "# NOTE: Execute the training loop. This will take around 8 minutes to complete.\n",
    "training_loop.run(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4\"></a>\n",
    "# Testing\n",
    "\n",
    "We will now be using the model you just trained to translate English sentences to German. We will implement this with two functions: The first allows you to identify the next symbol (i.e. output token). The second one takes care of combining the entire translated string.\n",
    "\n",
    "We will start by first loading in a pre-trained copy of the model you just coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the model we built in eval mode\n",
    "model = NMTAttn(mode='eval')\n",
    "\n",
    "# initialize weights from a pre-trained model\n",
    "model.init_from_file(\"model.pkl.gz\", weights_only=True)\n",
    "model = tl.Accelerate(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.1\"></a>\n",
    "## Decoding\n",
    "\n",
    "As discussed in the lectures, there are several ways to get the next token when translating a sentence. For instance, we can just get the most probable token at each step (i.e. greedy decoding) or get a sample from a distribution. We can generalize the implementation of these two approaches by using the `tl.logsoftmax_sample()` method. \n",
    "\n",
    "The key things to take away here are: 1. it gets random samples with the same shape as your input (i.e. `log_probs`), and 2. the amount of \"noise\" added to the input by these random samples is scaled by a `temperature` setting. You'll notice that setting it to `0` will just make the return statement equal to getting the argmax of `log_probs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature):\n",
    "    \"\"\"Returns the index of the next token.\n",
    "\n",
    "    Args:\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        input_tokens (np.ndarray 1 x n_tokens): tokenized representation of the input sentence\n",
    "        cur_output_tokens (list): tokenized representation of previously translated words\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "\n",
    "    Returns:\n",
    "        int: index of the next token in the translated sentence\n",
    "        float: log probability of the next symbol\n",
    "    \"\"\"\n",
    "\n",
    "    # set the length of the current output tokens\n",
    "    token_length = len(cur_output_tokens)\n",
    "\n",
    "    # calculate next power of 2 for padding length \n",
    "    padded_length = np.power(2, int(np.ceil(np.log2(token_length + 1))))\n",
    "\n",
    "    # pad cur_output_tokens up to the padded_length\n",
    "    padded = cur_output_tokens + [0] * (padded_length - token_length)\n",
    "    \n",
    "    # model expects the output to have an axis for the batch size in front so\n",
    "    # convert `padded` list to a numpy array with shape (x, <padded_length>) where the\n",
    "    # x position is the batch axis.\n",
    "    padded_with_batch = np.expand_dims(padded, axis = 0)\n",
    "\n",
    "    # get the model prediction. remember to use the `NMTAttn` argument defined above.\n",
    "    output, _ = NMTAttn((input_tokens, padded_with_batch))\n",
    "    #print(output)\n",
    "    #print(output.shape)\n",
    "    \n",
    "    # get log probabilities from the last token output\n",
    "    log_probs = output[0,token_length,:]\n",
    "\n",
    "    # get the next symbol by getting a logsoftmax sample (*hint: cast to an int)\n",
    "    symbol = int(tl.logsoftmax_sample(log_probs, temperature))\n",
    "    \n",
    "    return symbol, float(log_probs[symbol])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_next_symbol(next_symbol, model)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you will implement the `sampling_decode()` function. This will call the `next_symbol()` function above several times until the next output is the end-of-sentence token (i.e. `EOS`). It takes in an input string and returns the translated version of that string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling_decode(input_sentence, NMTAttn = None, temperature=0.0, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"Returns the translated sentence.\n",
    "\n",
    "    Args:\n",
    "        input_sentence (str): sentence to translate.\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "        vocab_file (str): filename of the vocabulary\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "\n",
    "    Returns:\n",
    "        tuple: (list, str, float)\n",
    "            list of int: tokenized version of the translated sentence\n",
    "            float: log probability of the translated sentence\n",
    "            str: the translated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    # encode the input sentence\n",
    "    input_tokens = tokenize(input_sentence,vocab_file,vocab_dir)\n",
    "    \n",
    "    # initialize the list of output tokens\n",
    "    cur_output_tokens = []\n",
    "    \n",
    "    # initialize an integer that represents the current output index\n",
    "    cur_output = 0\n",
    "    \n",
    "    # Set the encoding of the \"end of sentence\" as 1\n",
    "    EOS = 1\n",
    "    \n",
    "    # check that the current output is not the end of sentence token\n",
    "    while cur_output != EOS:\n",
    "        \n",
    "        # update the current output token by getting the index of the next word \n",
    "        cur_output, log_prob = next_symbol(NMTAttn, input_tokens, cur_output_tokens, temperature)\n",
    "        \n",
    "        # append the current output token to the list of output tokens\n",
    "        cur_output_tokens.append(cur_output)\n",
    "    \n",
    "    # detokenize the output tokens\n",
    "    sentence = detokenize(cur_output_tokens,vocab_file,vocab_dir)\n",
    "    \n",
    "    return cur_output_tokens, log_prob, sentence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([161, 12202, 5112, 3, 1], -0.0001735687255859375, 'Ich liebe Sprachen.')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the function above. Try varying the temperature setting with values from 0 to 1.\n",
    "# Run it several times with each setting and see how often the output changes.\n",
    "sampling_decode(\"I love languages.\", model, temperature=0.0, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_sampling_decode(sampling_decode, model)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have set a default value of `0` to the temperature setting in our implementation of `sampling_decode()` above. As you may have noticed in the `logsoftmax_sample()` method, this setting will ultimately result in greedy decoding. As mentioned in the lectures, this algorithm generates the translation by getting the most probable word at each step. It gets the argmax of the output array of your model and then returns that index. See the testing function and sample inputs below. You'll notice that the output will remain the same each time you run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode_test(sentence, NMTAttn=None, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"Prints the input and output of our NMTAttn model using greedy decode\n",
    "\n",
    "    Args:\n",
    "        sentence (str): a custom string.\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        vocab_file (str): filename of the vocabulary\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "\n",
    "    Returns:\n",
    "        str: the translated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    _,_, translated_sentence = sampling_decode(sentence, NMTAttn, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
    "    \n",
    "    print(\"English: \", sentence)\n",
    "    print(\"German: \", translated_sentence)\n",
    "    \n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  I love languages.\n",
      "German:  Ich liebe Sprachen.\n"
     ]
    }
   ],
   "source": [
    "# put a custom string here\n",
    "your_sentence = 'I love languages.'\n",
    "\n",
    "greedy_decode_test(your_sentence, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English:  You are almost done with the assignment!\n",
      "German:  Sie sind fast mit der Aufgabe fertig!\n"
     ]
    }
   ],
   "source": [
    "greedy_decode_test('You are almost done with the assignment!', model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"4.2\"></a>\n",
    "## Minimum Bayes-Risk Decoding\n",
    "\n",
    "As mentioned in the lectures, getting the most probable token at each step may not necessarily produce the best results. Another approach is to do Minimum Bayes Risk Decoding or MBR. The general steps to implement this are:\n",
    "\n",
    "1. take several random samples\n",
    "2. score each sample against all other samples\n",
    "3. select the one with the highest score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='4.2.1'></a>\n",
    "### 1 Generating samples\n",
    "\n",
    "First, let's build a function to generate several samples. We want to record the token list and log probability for each sample as these will be needed in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_samples(sentence, n_samples, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"Generates samples using sampling_decode()\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to translate.\n",
    "        n_samples (int): number of samples to generate\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "        vocab_file (str): filename of the vocabulary\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (list, list)\n",
    "            list of lists: token list per sample\n",
    "            list of floats: log probability per sample\n",
    "    \"\"\"\n",
    "    # define lists to contain samples and probabilities\n",
    "    samples, log_probs = [], []\n",
    "\n",
    "    # run a for loop to generate n samples\n",
    "    for _ in range(n_samples):\n",
    "        \n",
    "        # get a sample\n",
    "        sample, logp, _ = sampling_decode(sentence, NMTAttn, temperature, vocab_file=vocab_file, vocab_dir=vocab_dir)\n",
    "        \n",
    "        # append the token list to the samples list\n",
    "        samples.append(sample)\n",
    "        \n",
    "        # append the log probability to the log_probs list\n",
    "        log_probs.append(logp)\n",
    "                \n",
    "    return samples, log_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([[161, 12202, 5112, 3, 1],\n",
       "  [161, 12202, 5112, 3, 1],\n",
       "  [161, 12202, 10, 5112, 3, 1],\n",
       "  [161, 12202, 5112, 3, 1]],\n",
       " [-0.0001735687255859375,\n",
       "  -0.0001735687255859375,\n",
       "  -0.0001087188720703125,\n",
       "  -0.0001735687255859375])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# generate 4 samples with the default temperature (0.6)\n",
    "generate_samples('I love languages.', 4, model, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 Comparing overlaps\n",
    "\n",
    "Let us now build our functions to compare a sample against another. There are several metrics available as shown in the lectures and you can try experimenting with any one of these. For this assignment, we will be calculating scores for unigram overlaps. One of the more simple metrics is the [Jaccard similarity](https://en.wikipedia.org/wiki/Jaccard_index) which gets the intersection over union of two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity(candidate, reference):\n",
    "    \"\"\"Returns the Jaccard similarity between two token lists\n",
    "\n",
    "    Args:\n",
    "        candidate (list of int): tokenized version of the candidate translation\n",
    "        reference (list of int): tokenized version of the reference translation\n",
    "\n",
    "    Returns:\n",
    "        float: overlap between the two token lists\n",
    "    \"\"\"\n",
    "    \n",
    "    # convert the lists to a set to get the unique tokens\n",
    "    can_unigram_set, ref_unigram_set = set(candidate), set(reference)  \n",
    "    \n",
    "    # get the set of tokens common to both candidate and reference\n",
    "    joint_elems = can_unigram_set.intersection(ref_unigram_set)\n",
    "    \n",
    "    # get the set of all tokens found in either candidate or reference\n",
    "    all_elems = can_unigram_set.union(ref_unigram_set)\n",
    "    \n",
    "    # divide the number of joint elements by the number of all elements\n",
    "    overlap = len(joint_elems) / len(all_elems)\n",
    "    \n",
    "    return overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try using the function. remember the result here and compare with the next function below.\n",
    "jaccard_similarity([1, 2, 3], [1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the more commonly used metrics in machine translation is the ROUGE score. For unigrams, this is called ROUGE-1 and as shown in class, you can output the scores for both precision and recall when comparing two samples. To get the final score, you will want to compute the F1-score as given by:\n",
    "\n",
    "$$score = 2* \\frac{(precision * recall)}{(precision + recall)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for making a frequency table easily\n",
    "from collections import Counter\n",
    "\n",
    "def rouge1_similarity(system, reference):\n",
    "    \"\"\"Returns the ROUGE-1 score between two token lists\n",
    "\n",
    "    Args:\n",
    "        system (list of int): tokenized version of the system translation\n",
    "        reference (list of int): tokenized version of the reference translation\n",
    "\n",
    "    Returns:\n",
    "        float: overlap between the two token lists\n",
    "    \"\"\"    \n",
    "    \n",
    "    # make a frequency table of the system tokens\n",
    "    sys_counter = Counter(system)\n",
    "    \n",
    "    # make a frequency table of the reference tokens\n",
    "    ref_counter = Counter(reference)\n",
    "    \n",
    "    # initialize overlap to 0\n",
    "    overlap = 0\n",
    "    \n",
    "    # run a for loop over the sys_counter object (can be treated as a dictionary)\n",
    "    for token in sys_counter:\n",
    "        \n",
    "        # lookup the value of the token in the sys_counter dictionary (hint: use the get() method)\n",
    "        token_count_sys = sys_counter.get(token,0)\n",
    "        \n",
    "        # lookup the value of the token in the ref_counter dictionary (hint: use the get() method)\n",
    "        token_count_ref = ref_counter.get(token,0)\n",
    "        \n",
    "        # update the overlap by getting the smaller number between the two token counts above\n",
    "        overlap +=  min(token_count_sys,token_count_ref)\n",
    "    \n",
    "    # get the precision (i.e. number of overlapping tokens / number of system tokens)\n",
    "    precision = overlap / sum(sys_counter.values()) \n",
    "    \n",
    "    # get the recall (i.e. number of overlapping tokens / number of reference tokens)\n",
    "    recall = overlap / sum(ref_counter.values())\n",
    "    \n",
    "    if precision + recall != 0:\n",
    "        # compute the f1-score\n",
    "        rouge1_score = 2 * precision * recall / (precision + recall)\n",
    "    else:\n",
    "        rouge1_score = 0 \n",
    "    \n",
    "    return rouge1_score\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8571428571428571"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# notice that this produces a different value from the jaccard similarity earlier\n",
    "rouge1_similarity([1, 2, 3], [1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_rouge1_similarity(rouge1_similarity)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 Overall score\n",
    "\n",
    "We will now build a function to generate the overall score for a particular sample. As mentioned earlier, we need to compare each sample with all other samples. For instance, if we generated 30 sentences, we will need to compare sentence 1 to sentences 2 to 30. Then, we compare sentence 2 to sentences 1 and 3 to 30, and so forth. At each step, we get the average score of all comparisons to get the overall score for a particular sample. To illustrate, these will be the steps to generate the scores of a 4-sample list.\n",
    "\n",
    "1. Get similarity score between sample 1 and sample 2\n",
    "2. Get similarity score between sample 1 and sample 3\n",
    "3. Get similarity score between sample 1 and sample 4\n",
    "4. Get average score of the first 3 steps. This will be the overall score of sample 1.\n",
    "5. Iterate and repeat until samples 1 to 4 have overall scores.\n",
    "\n",
    "We will be storing the results in a dictionary for easy lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_overlap(similarity_fn, samples, *ignore_params):\n",
    "    \"\"\"Returns the arithmetic mean of each candidate sentence in the samples\n",
    "\n",
    "    Args:\n",
    "        similarity_fn (function): similarity function used to compute the overlap\n",
    "        samples (list of lists): tokenized version of the translated sentences\n",
    "        *ignore_params: additional parameters will be ignored\n",
    "\n",
    "    Returns:\n",
    "        dict: scores of each sample\n",
    "            key: index of the sample\n",
    "            value: score of the sample\n",
    "    \"\"\"  \n",
    "    \n",
    "    # initialize dictionary\n",
    "    scores = {}\n",
    "    \n",
    "    # run a for loop for each sample\n",
    "    for index_candidate, candidate in enumerate(samples):    \n",
    "        overlap = 0\n",
    "        \n",
    "        # run a for loop for each sample\n",
    "        for index_sample, sample in enumerate(samples): \n",
    "\n",
    "            # skip if the candidate index is the same as the sample index\n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "                \n",
    "            # get the overlap between candidate and sample using the similarity function\n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "            \n",
    "            # add the sample overlap to the total overlap\n",
    "            overlap += sample_overlap\n",
    "            \n",
    "        # get the score for the candidate by computing the average\n",
    "        score = overlap / (len(samples) - 1)\n",
    "        \n",
    "        # save the score in the dictionary. use index as the key.\n",
    "        scores[index_candidate] = score\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.45, 1: 0.625, 2: 0.575}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_average_overlap(average_overlap)\n",
    "# END UNIT TEST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, it is also common to see the weighted mean being used to calculate the overall score instead of just the arithmetic mean. We have implemented it below and you can use it in your experiements to see which one will give better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_avg_overlap(similarity_fn, samples, log_probs):\n",
    "    \"\"\"Returns the weighted mean of each candidate sentence in the samples\n",
    "\n",
    "    Args:\n",
    "        samples (list of lists): tokenized version of the translated sentences\n",
    "        log_probs (list of float): log probability of the translated sentences\n",
    "\n",
    "    Returns:\n",
    "        dict: scores of each sample\n",
    "            key: index of the sample\n",
    "            value: score of the sample\n",
    "    \"\"\"\n",
    "    \n",
    "    # initialize dictionary\n",
    "    scores = {}\n",
    "    \n",
    "    # run a for loop for each sample\n",
    "    for index_candidate, candidate in enumerate(samples):    \n",
    "        \n",
    "        # initialize overlap and weighted sum\n",
    "        overlap, weight_sum = 0.0, 0.0\n",
    "        \n",
    "        # run a for loop for each sample\n",
    "        for index_sample, (sample, logp) in enumerate(zip(samples, log_probs)):\n",
    "\n",
    "            # skip if the candidate index is the same as the sample index            \n",
    "            if index_candidate == index_sample:\n",
    "                continue\n",
    "                \n",
    "            # convert log probability to linear scale\n",
    "            sample_p = float(np.exp(logp))\n",
    "\n",
    "            # update the weighted sum\n",
    "            weight_sum += sample_p\n",
    "\n",
    "            # get the unigram overlap between candidate and sample\n",
    "            sample_overlap = similarity_fn(candidate, sample)\n",
    "            \n",
    "            # update the overlap\n",
    "            overlap += sample_p * sample_overlap\n",
    "            \n",
    "        # get the score for the candidate\n",
    "        score = overlap / weight_sum\n",
    "        \n",
    "        # save the score in the dictionary. use index as the key.\n",
    "        scores[index_candidate] = score\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.44255574831883415, 1: 0.631244796869735, 2: 0.5575581009406329}"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_avg_overlap(jaccard_similarity, [[1, 2, 3], [1, 2, 4], [1, 2, 4, 5]], [0.4, 0.2, 0.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it all together\n",
    "\n",
    "We will now put everything together and develop the `mbr_decode()` function. Generate samples, get the score for each sample, get the highest score among all samples, then detokenize this sample to get the translated sentence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mbr_decode(sentence, n_samples, score_fn, similarity_fn, NMTAttn=None, temperature=0.6, vocab_file=None, vocab_dir=None):\n",
    "    \"\"\"Returns the translated sentence using Minimum Bayes Risk decoding\n",
    "\n",
    "    Args:\n",
    "        sentence (str): sentence to translate.\n",
    "        n_samples (int): number of samples to generate\n",
    "        score_fn (function): function that generates the score for each sample\n",
    "        similarity_fn (function): function used to compute the overlap between a pair of samples\n",
    "        NMTAttn (tl.Serial): An LSTM sequence-to-sequence model with attention.\n",
    "        temperature (float): parameter for sampling ranging from 0.0 to 1.0.\n",
    "            0.0: same as argmax, always pick the most probable token\n",
    "            1.0: sampling from the distribution (can sometimes say random things)\n",
    "        vocab_file (str): filename of the vocabulary\n",
    "        vocab_dir (str): path to the vocabulary file\n",
    "\n",
    "    Returns:\n",
    "        str: the translated sentence\n",
    "    \"\"\"\n",
    "    \n",
    "    # generate samples\n",
    "    samples, log_probs = generate_samples(sentence, n_samples, NMTAttn, temperature, vocab_file, vocab_dir)\n",
    "    \n",
    "    # use the scoring function to get a dictionary of scores\n",
    "    # pass in the relevant parameters as shown in the function definition of \n",
    "    # the mean methods you developed earlier\n",
    "    scores = weighted_avg_overlap(similarity_fn, samples, log_probs)\n",
    "    \n",
    "    # find the key with the highest score\n",
    "    max_index = max(scores, key=scores.get)\n",
    "    # detokenize the token list associated with the max_index\n",
    "    translated_sentence = detokenize(samples[max_index], vocab_file, vocab_dir)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return (translated_sentence, max_index, scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPERATURE = 1.0\n",
    "\n",
    "# put a custom string here\n",
    "your_sentence = 'She speaks English and German.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sie spricht Englisch und Französisch.'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbr_decode(your_sentence, 4, weighted_avg_overlap, jaccard_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Herzlichen Glückwunsch!'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbr_decode('Congratulations!', 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Sie haben das Gebot geschafft!'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mbr_decode('You have completed the assignment!', 4, average_overlap, rouge1_similarity, model, TEMPERATURE, vocab_file=VOCAB_FILE, vocab_dir=VOCAB_DIR)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This unit test take a while to run. Please be patient**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m All tests passed\n"
     ]
    }
   ],
   "source": [
    "# BEGIN UNIT TEST\n",
    "w1_unittest.test_mbr_decode(mbr_decode, model)\n",
    "# END UNIT TEST"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "schema_names": [
    "NLPC4-1"
   ]
  },
  "jupytext": {
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
